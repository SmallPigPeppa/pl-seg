{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766cadc-cc3b-41e4-848f-76c78f16d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce36537-fa13-4fb3-9ada-ca551ba96c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "from typing import List, Union, Tuple\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "\n",
    "from segmentation.model import SegmentationModel\n",
    "from segmentation.train_utils import benchmark_inference_time, save_model_to_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ae123-0a27-454c-8a4e-4817b16d4c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT=\"CamVid\"\n",
    "ENTITY=\"av-demo\"\n",
    "IMAGE_SHAPE = (720, 960)\n",
    "SEED = 123\n",
    "RUN_NAME = \"baseline-train-1\"\n",
    "JOB_TYPE = \"baseline-train\"\n",
    "\n",
    "ARTIFACT_ID = \"av-demo/CamVid/camvid-dataset:v0\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_RESIZE_FACTOR = 4\n",
    "VALIDATION_SPLIT = 0.2\n",
    "HIDDEN_DIM = 256\n",
    "BACKBONE = \"mobilenetv2_100\"\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAIN_EPOCHS = 10\n",
    "\n",
    "INFERENCE_BATCH_SIZE = 8\n",
    "NUM_WARMUP_ITERS = 10\n",
    "NUM_INFERENCE_BENCHMARK_ITERS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d72582-8a20-440e-8909-8549c8196766",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf22ef-00d9-43b0-abdc-903df6012dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=PROJECT,\n",
    "    name=RUN_NAME,\n",
    "    entity=ENTITY,\n",
    "    job_type=JOB_TYPE,\n",
    "    config={\n",
    "        \"artifact_id\": ARTIFACT_ID,\n",
    "        \"image_shape\": IMAGE_SHAPE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"image_resize_factor\": IMAGE_RESIZE_FACTOR,\n",
    "        \"validation_split\": VALIDATION_SPLIT,\n",
    "        \"hidden_dims\": HIDDEN_DIM,\n",
    "        \"backbone\": BACKBONE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"train_epochs\": TRAIN_EPOCHS,\n",
    "        \"inference_batch_size\": INFERENCE_BATCH_SIZE,\n",
    "        \"num_warmup_iters\": NUM_WARMUP_ITERS,\n",
    "        \"num_inference_banchmark_iters\": NUM_INFERENCE_BENCHMARK_ITERS\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03ddbe-0258-4d27-b393-dcfb1576e4f8",
   "metadata": {},
   "source": [
    "## DataLoader for SegmentationDataLoader for Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5880664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(fn):\n",
    "    return fn.parent.parent/\"labels\"/f\"{fn.stem}_P{fn.suffix}\"\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    artifact_id: str,\n",
    "    batch_size: int,\n",
    "    image_shape: Tuple[int, int],\n",
    "    resize_factor: int,\n",
    "    validation_split: float,\n",
    "    seed: int\n",
    "):\n",
    "    \"\"\"Grab an artifact and creating a Pytorch DataLoader\"\"\"\n",
    "    artifact = wandb.use_artifact(artifact_id, type='dataset')\n",
    "    artifact_dir = Path(artifact.download())\n",
    "    codes = np.loadtxt(artifact_dir/'codes.txt', dtype=str)\n",
    "    fnames = get_image_files(artifact_dir/\"images\")\n",
    "    class_labels = {k: v for k, v in enumerate(codes)}\n",
    "    return SegmentationDataLoaders.from_label_func(\n",
    "        artifact_dir,\n",
    "        bs=batch_size,\n",
    "        fnames=fnames,\n",
    "        label_func=label_func,\n",
    "        codes=codes,\n",
    "        item_tfms=Resize((\n",
    "            image_shape[0] // resize_factor,\n",
    "            image_shape[1] // resize_factor\n",
    "        )),\n",
    "        valid_pct=validation_split,\n",
    "        seed=seed\n",
    "    ), class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3813596-79cb-4784-b1a0-01f18b1aaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader, class_labels = get_dataloader(\n",
    "    artifact_id=ARTIFACT_ID,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_shape=IMAGE_SHAPE,\n",
    "    resize_factor=IMAGE_RESIZE_FACTOR,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "data_loader.show_batch(max_n=4, vmin=1, vmax=30, figsize=(14, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219ae16-e20d-41de-bfb9-8c78eef3af8d",
   "metadata": {},
   "source": [
    "## Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15504108-fc34-4f14-9b79-5e15a626fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameters(model):\n",
    "    with torch.no_grad():\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "    return num_params\n",
    "\n",
    "\n",
    "def get_predictions(learner):\n",
    "    inputs, predictions, targets, outputs = learner.get_preds(with_input=True, with_decoded=True)\n",
    "    x, y, samples, outputs = learner.dls.valid.show_results(\n",
    "        tuplify(inputs) + tuplify(targets), outputs, show=False, max_n=36\n",
    "    )\n",
    "    return samples, outputs, predictions\n",
    "\n",
    "\n",
    "def create_wandb_table(samples, outputs, class_labels):\n",
    "    \"Creates a wandb table with predictions and targets side by side\"\n",
    "    table = wandb.Table(columns=[\"Image\", \"Predicted Mask\", \"Ground Truth\"])\n",
    "    for (image, label), pred_label in zip(samples, outputs):\n",
    "        image = image.permute(1, 2, 0)\n",
    "        table.add_data(\n",
    "            wandb.Image(image),\n",
    "            wandb.Image(\n",
    "                image,\n",
    "                masks={\n",
    "                    \"predictions\":  {\n",
    "                        'mask_data':  pred_label[0].numpy(),\n",
    "                        'class_labels':class_labels\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            wandb.Image(\n",
    "                image,\n",
    "                masks={\n",
    "                    \"ground truths\": {\n",
    "                        'mask_data': label.numpy(),\n",
    "                        'class_labels':class_labels\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8512276-b961-4bed-91bb-55149ab3337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(\n",
    "    data_loader,\n",
    "    backbone: str,\n",
    "    hidden_dim: int,\n",
    "    num_classes: int,\n",
    "    checkpoint_file: Union[None, str, Path],\n",
    "    loss_func,\n",
    "    metrics: List,\n",
    "    log_preds: bool = False\n",
    "):\n",
    "    model = SegmentationModel(backbone, hidden_dim, num_classes=num_classes)\n",
    "    save_model_callback = SaveModelCallback(fname=f\"unet_{backbone}\")\n",
    "    mixed_precision_callback = MixedPrecision()\n",
    "    wandb_callback = WandbCallback(log_preds=log_preds)\n",
    "    learner = Learner(\n",
    "        data_loader,\n",
    "        model,\n",
    "        loss_func=loss_func,\n",
    "        metrics=metrics,\n",
    "        cbs=[save_model_callback, mixed_precision_callback, wandb_callback],\n",
    "    )\n",
    "    if checkpoint_file is not None:\n",
    "        learner.load(checkpoint_file)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1b301-3a35-4c88-8271-6532f533cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = get_learner(\n",
    "    data_loader,\n",
    "    backbone=BACKBONE,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=len(class_labels),\n",
    "    checkpoint_file=None,\n",
    "    loss_func=FocalLossFlat(axis=1),\n",
    "    metrics=[DiceMulti(), foreground_acc],\n",
    "    log_preds=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b077e-01f3-48c6-be41-8a316870cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.fine_tune(10, 1e-3)\n",
    "learner.fit_one_cycle(TRAIN_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296f17b-e944-4a83-9b6c-26883147e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.show_results(max_n=4, figsize=(14, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6da99-127d-46ad-bd92-37900d08e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, outputs, _ = get_predictions(learner)\n",
    "table = create_wandb_table(samples, outputs, class_labels)\n",
    "wandb.log({f\"Baseline_Predictions_{run.name}\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccf68e-6f85-42da-b234-7fa08d289f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "wandb.log({\"Model_Parameters\": get_model_parameters(model)})\n",
    "wandb.log({\n",
    "    \"Inference_Time\": benchmark_inference_time(\n",
    "        model=model,\n",
    "        batch_size=INFERENCE_BATCH_SIZE,\n",
    "        image_shape=IMAGE_SHAPE,\n",
    "        num_warmup_iters=NUM_WARMUP_ITERS,\n",
    "        num_iter=NUM_INFERENCE_BENCHMARK_ITERS,\n",
    "        seed=SEED\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098458fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_to_artifacts(\n",
    "    model=model,\n",
    "    model_name=f\"unet_{BACKBONE}\",\n",
    "    image_shape=IMAGE_SHAPE,\n",
    "    artifact_name=f\"{run.name}-saved-model\",\n",
    "    metadata={\n",
    "        \"backbone\": BACKBONE,\n",
    "        \"hidden_dims\": HIDDEN_DIM,\n",
    "        \"input_size\": IMAGE_SHAPE,\n",
    "        \"class_labels\": class_labels\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf7bf8-bb74-4464-b562-04cc3b12ac21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d4064-46bb-4dbe-9da1-e155d19d3794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
